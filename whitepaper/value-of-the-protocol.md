# Value of the Protocol

For the past 10 years, the geospatial community has been building and solidifying a native ecosystem in the web2 space. AWS S3, AWS Lambda, GCP BigQuery GIS, to name a few. The go-to standard for storing, retrieving, processing, and analyzing geospatial data, has traditionally been with cloud service providers.

As a result, most of the tooling, workflows, specifications, and projects have been built to compliment the web2 space. The Spatio Temporal Asset Catalog\(STAC\) specification and the Cloud Optimized GeoTIFFs \(COGs\) are two of the leading specifications/standards used within the geospatial industry.

STAC is supported by an active community of developers, with involvement from a large range of organizations, including Radiant Earth Foundation, Microsoft, Google Earth Engine, Near Space Labs, L3Harris, etc. In addition, COGs are gaining widespread adoption from the likes of Raster Foundry, GDAL, Google Earth Engine, QGIS, and interoperability with Amazon S3.

The problem is that the aforementioned specification and file standard are only beneficial for the web2 ecosystem. They are optimized and designed to leverage cloud architecture, and location based addresses. When porting these to web3, we see that they become useless because they are not interoperable with CIDs and distributed file systems, as they do not offer web3 leveraged features.

The value we want to provide to the web3 ecosystem is by building tools, specifications and projects that will enable the geospatial community to further leverage our products as an alternative to the traditional centralized options. We believe that we can give the geospatial community cheaper, faster and better solutions for hosting, sharing and/or retrieving geospatial data.

We then plan to bring traditional geospatial tools and specifications to the web3 universe, by creating geoDIDs to extend the STAC Specification and enabling IPLD encoded geoTIFFs to bring byte served tile imagery to the IPFS ecosystem.

Note in the chart below that there is an exponential annual increase of spatial data being uploaded to cloud storage services. Itâ€™s only a matter of time before people within this community will want to leverage this type of data within a distributed file system like IPFS, and/or utilize the data to create innovative applications within the web3 space.

![Yearly data volume increase, 2013-2019. Big Earth Data: From Data to Information](https://camo.githubusercontent.com/6371f0fbc1ed79a0076d6f7d126dde45519e38826d3c8834c49fbf266c2cc870/68747470733a2f2f6172732e656c732d63646e2e636f6d2f636f6e74656e742f696d6167652f312d73322e302d53303136373733395831373330303738582d6772312e6a7067)

The applications enabled by the spatial data layer of the decentralized web are wide-ranging and revolutionary. We are learning by building - we built a [prototype sustainability-linked bond](https://github.com/AstralProtocol/sprout) on Ethereum and IPFS, which aligns financial and ecological incentives by adjusting the amount a borrower needs to pay each year based on a measurement of environmental health - in our case, analytics derived from raster images of air quality in London.

We've also designed a spatial governance protocol for connected devices - a system for applying policies to vehicles and other devices based on their geographic position in zones, represented by vector geometries - while preserving the privacy of everyone involved. We built [Hyperaware](https://hyperaware.io/), an alpha implementation of the protocol, on Arweave, IoTeX and Ethereum.

These projects have a common requirement: the capability to work with spatial data on the decentralized web. But first we need to develop the specifications and tools needed to work with this type of data in the web3 space, without having to opt for a web2 alternative. Once we develop this, we open a brand new world of possibilities within the web3 space, that will allow: developers to leverage geospatial data within their applications; data providers to store and distribute their data efficiently and effectively; data scientists to manipulate, analyze, and share their findings in a more user friendly way.

